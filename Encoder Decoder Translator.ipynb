{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a6ec68b",
   "metadata": {},
   "source": [
    "Basic simple encoder decoder net.\n",
    "Could be expanded to become a fully fledged translator.\n",
    "\n",
    "This should be able to translate any set of token pairs. The first column should contain the input and the second column the target.\n",
    "\n",
    "Improvement ideas:\n",
    "* Use embeddings instead of one hot vector representation\n",
    "* Do not pass the target to the seq2seq during evaluation\n",
    "* Try using teacher forcing some % of the time\n",
    "* Generates the max length sequence - maybe stop when hit <end>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "04c4bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "c962afba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "filename = 'nums.txt'  # Replace with your file name\n",
    "\n",
    "nums =[]\n",
    "words=[]\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        nums.append(row[0])\n",
    "        words.append(row[1])\n",
    "\n",
    "zipped = zip(nums, words)\n",
    "\n",
    "pairs=[]\n",
    "for item in zipped:\n",
    "    n,w = item    \n",
    "    pairs.append([\"<s> \"+n+\" <e>\",\"<s> \"+w+\" <e>\"])\n",
    "    \n",
    "print(\"Num examples:\", len(pairs))\n",
    "\n",
    "\n",
    "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
    "# (e.g., 5 -> \"dad\") for each language,\n",
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        self.create_index()\n",
    "\n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "\n",
    "        self.vocab = sorted(self.vocab)\n",
    "\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        \n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word\n",
    "            \n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def load_dataset(pairs, num_examples):\n",
    "    # pairs => already created cleaned input, output pairs\n",
    "\n",
    "    # index language using the class defined above\n",
    "    inp_lang = LanguageIndex(n for n, w in pairs)\n",
    "    targ_lang = LanguageIndex(w for n, w in pairs)\n",
    "\n",
    "    # Vectorize the input and target languages\n",
    "\n",
    "    # Input sentences\n",
    "    input_tensor = [torch.tensor([inp_lang.word2idx[s] for s in en.split(' ')], dtype=torch.long) for en, ma in pairs]\n",
    "\n",
    "    # Target sentences\n",
    "    target_tensor = [torch.tensor([targ_lang.word2idx[s] for s in ma.split(' ')], dtype=torch.long) for en, ma in pairs]\n",
    "\n",
    "    # Calculate max_length of input and output tensor\n",
    "    # Here, we'll set those to the longest sentence in the dataset\n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "\n",
    "    # Padding the input and output tensor to the maximum length\n",
    "    input_tensor = pad_sequence(input_tensor, batch_first=True, padding_value=0)\n",
    "    input_tensor = input_tensor[:, :max_length_inp]\n",
    "\n",
    "    target_tensor = pad_sequence(target_tensor, batch_first=True, padding_value=0)\n",
    "    target_tensor = target_tensor[:, :max_length_tar]\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar\n",
    "\n",
    "\n",
    "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(pairs, len(pairs))\n",
    "\n",
    "# One hot tensors -> could move to an embedding layer\n",
    "oh_input = F.one_hot(input_tensor)\n",
    "oh_target = F.one_hot(target_tensor)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "5bb8c43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT VOCAB SIZE:14, TARGET VOCAB SIZE: 33, UNITS: 128,        INPUT SEQLEN:5, TARGET SEQ LENGTH: 6\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS AND HYPERPARAMETERS \n",
    "INP_VOCAB_SIZE = len(inp_lang.word2idx)\n",
    "TAR_VOCAB_SIZE = len(targ_lang.word2idx)\n",
    "UNITS = 128 # #of units in the GRU - both encoder and decoder use this setting\n",
    "INP_SEQ_LEN = max_length_inp\n",
    "TAR_SEQ_LEN = max_length_targ\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "print(f\"INPUT VOCAB SIZE:{INP_VOCAB_SIZE}, TARGET VOCAB SIZE: {TAR_VOCAB_SIZE}, UNITS: {UNITS},\\\n",
    "        INPUT SEQLEN:{INP_SEQ_LEN}, TARGET SEQ LENGTH: {TAR_SEQ_LEN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "28ab30b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.055296 M parameters\n",
      "0.066849 M parameters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Wrap the tensors in a TensorDataset and create the loader\n",
    "dataset = TensorDataset(oh_input, oh_target)\n",
    "\n",
    "# Split the dataset into train and eval subsets\n",
    "train_size = int(0.9 * len(dataset))  # 90% for training\n",
    "eval_size = len(dataset) - train_size  # 10% for evaluation\n",
    "train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "# Create DataLoaders for training and evaluation\n",
    "dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "evalloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "# Encoder class \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Encoder GRU\n",
    "        self.encoder_gru = nn.GRU(input_size, hidden_size, batch_first=True)    \n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # input_seq: [batch, seq, input_vocab]\n",
    "        # outout : [batch, seq, units]\n",
    "        # hidden: [1, seq, units] or [numlayers, seq, units]\n",
    "        encoder_hidden = self.initialize_hidden(input_seq.shape[0])\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_output, encoder_hidden = self.encoder_gru(input_seq, encoder_hidden)\n",
    "        return encoder_output, encoder_hidden\n",
    "    \n",
    "    def initialize_hidden(self, batchlen):\n",
    "        return torch.zeros(1, batchlen, self.hidden_size)\n",
    "\n",
    "# Decoder class\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Decoder GRU\n",
    "        self.decoder_gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        # Linear layer to map hidden state to output\n",
    "        self.fc = nn.Linear(hidden_size, output_size) \n",
    "\n",
    "    def forward(self, input_seq, hidden):\n",
    "        # input_seq: [batch, seq (1), input_vocab]\n",
    "        # outout : [batch, seq, units]\n",
    "        # hidden: [1, seq, units] or [numlayers, seq, units]\n",
    "\n",
    "        dec_output, dec_hidden = self.decoder_gru(input_seq, hidden)\n",
    "        # dec_output shape: 1, seq_len, hidden_size \n",
    "        \n",
    "        dec_output = self.fc(dec_output)\n",
    "        # output shape: 1, seq_len, output_size\n",
    "        return dec_output, dec_hidden\n",
    "\n",
    "    \n",
    "# The Seq2Seq model to connect the encoder and decoder\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    \n",
    "    def forward(self, source, target, batchsize, training):\n",
    "        # The sequence we produce will be stored in this tensor\n",
    "        outputs = torch.zeros(batchsize, TAR_SEQ_LEN , TAR_VOCAB_SIZE)\n",
    "        #print(\"outputs storage shape:\", outputs.shape)\n",
    "        \n",
    "        # Call the encoder\n",
    "        enc_output, enc_hidden = self.encoder(source)\n",
    "        # Grab the last hidden layer for the decoder\n",
    "        # enc_output shape: [batch, seq, units]\n",
    "        # hidden shape: [1, seq, units] or [numlayers, seq, units] \n",
    "        \n",
    "        # Grab the last time slice of the hidden units\n",
    "        hidden = enc_hidden\n",
    "        # hidden shape: [1, seq, units]\n",
    "        \n",
    "        # Grab the first input to the Decoder which will be the <s> token\n",
    "        output = seed_oh = F.one_hot(torch.tensor(targ_lang.word2idx['<s>']), TAR_VOCAB_SIZE).unsqueeze(0).float()\n",
    "        output = output.unsqueeze(0)\n",
    "        output = output.repeat(batchsize,1,1)\n",
    "        \n",
    "        # Generate the full sequence -> loop could be improved to a while loop\n",
    "        for t in range(TAR_SEQ_LEN):\n",
    "            #if(training):\n",
    "            #    #Teacher forcing only if during training\n",
    "            #    x = target[:,t,:].unsqueeze(1)\n",
    "            #    #print(\"Train x shape:\", x.shape)\n",
    "            #else:\n",
    "            x = output\n",
    "\n",
    "            # Use previous hidden, cell as context from encoder at start\n",
    "            output, hidden = self.decoder(x, hidden)\n",
    "            # output shape: 1, seq_len, output_size\n",
    "            \n",
    "            # Store next output prediction\n",
    "            outputs[:,t,:] = output.squeeze()           \n",
    "            # outputs shape: seq_len, output_size\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "    \n",
    "# Build the model and print out some stats    \n",
    "encoder = Encoder(input_size=INP_VOCAB_SIZE, hidden_size=UNITS)\n",
    "decoder = Decoder(input_size=TAR_VOCAB_SIZE, hidden_size=UNITS, output_size=TAR_VOCAB_SIZE)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "\n",
    "print(sum(p.numel() for p in encoder.parameters())/1e6, 'M parameters')\n",
    "print(sum(p.numel() for p in decoder.parameters())/1e6, 'M parameters')\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "37435da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 52.50809860229492\n",
      "Loss: 27.930728912353516\n",
      "Loss: 22.784717559814453\n",
      "Loss: 19.416542053222656\n",
      "Loss: 17.342666625976562\n",
      "Loss: 16.163410186767578\n",
      "Loss: 15.43964958190918\n",
      "Loss: 14.698653221130371\n",
      "Loss: 13.88443660736084\n",
      "Loss: 13.04961109161377\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Training phase\n",
    "\n",
    "# for param_group in optimizer.param_groups:\n",
    "#     param_group['lr'] = 0.01\n",
    "\n",
    "bsize = 1\n",
    "\n",
    "EPOCHS = 500\n",
    "model.train()\n",
    "\n",
    "for e in range(EPOCHS):    \n",
    "    loss =0 \n",
    "    for index, (inp, targ) in enumerate(dataloader):\n",
    "\n",
    "        bsize = inp.shape[0]\n",
    "        \n",
    "        output = model(inp.float(), targ.float(), bsize, 1)\n",
    "        \n",
    "        #print(\"model output:\", output.shape)\n",
    "        output = output.squeeze(1)\n",
    "        targ = torch.argmax(targ, dim=2).squeeze(0)\n",
    "        \n",
    "        outr = output.view(-1,TAR_VOCAB_SIZE)\n",
    "        tarr = targ.view(-1)\n",
    "        loss += criterion(outr,tarr)\n",
    "\n",
    "    if(e%(EPOCHS/10)==0):\n",
    "        print(\"Loss:\", loss.item())\n",
    "\n",
    "    # Training time ...\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "fa50e720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <s> 4 7 7 <e> Pred: <s> seven hundred fifty seven <e> Target: <s> four hundred seventy seven <e> \n",
      "Input: <s> 6 8 3 <e> Pred: <s> nine hundred ninety nine <e> Target: <s> six hundred eighty three <e> \n",
      "Input: <s> 7 8 1 <e> Pred: <s> seven hundred twenty one <e> Target: <s> seven hundred eighty one <e> \n",
      "Input: <s> 7 6 2 <e> Pred: <s> seven hundred twenty two <e> Target: <s> seven hundred sixty two <e> \n",
      "Input: <s> 1 0 5 <e> Pred: <s> one hundred four <e> <pad> Target: <s> one hundred five <e> <pad> \n",
      "Input: <s> 2 5 9 <e> Pred: <s> four hundred ninety nine <e> Target: <s> two hundred fifty nine <e> \n",
      "Input: <s> 8 9 <e> <pad> Pred: <s> thirty three <e> <pad> <pad> Target: <s> eighty nine <e> <pad> <pad> \n",
      "Input: <s> 8 4 2 <e> Pred: <s> nine hundred ninety seven <e> Target: <s> eight hundred forty two <e> \n",
      "Input: <s> 3 7 0 <e> Pred: <s> nine hundred fifty  <e> Target: <s> three hundred seventy  <e> \n",
      "Input: <s> 1 6 2 <e> Pred: <s> one hundred twenty two <e> Target: <s> one hundred sixty two <e> \n",
      "Input: <s> 2 7 4 <e> Pred: <s> seven hundred twenty nine <e> Target: <s> two hundred seventy four <e> \n",
      "Input: <s> 3 4 7 <e> Pred: <s> nine hundred ninety seven <e> Target: <s> three hundred forty seven <e> \n",
      "Input: <s> 4 8 3 <e> Pred: <s> four hundred twenty nine <e> Target: <s> four hundred eighty three <e> \n",
      "Input: <s> 4 4 0 <e> Pred: <s> nine hundred fifty  <e> Target: <s> four hundred forty  <e> \n",
      "Input: <s> 3 1 9 <e> Pred: <s> nine hundred fifteen <e> <pad> Target: <s> three hundred nineteen <e> <pad> \n",
      "Input: <s> 5 1 4 <e> Pred: <s> nine hundred sixteen <e> <pad> Target: <s> five hundred fourteen <e> <pad> \n",
      "Input: <s> 7 1 4 <e> Pred: <s> one hundred sixteen <e> <pad> Target: <s> seven hundred fourteen <e> <pad> \n",
      "Input: <s> 1 0 3 <e> Pred: <s> one hundred four <e> <pad> Target: <s> one hundred three <e> <pad> \n",
      "Input: <s> 4 2 1 <e> Pred: <s> seven hundred twenty one <e> Target: <s> four hundred twenty one <e> \n",
      "Input: <s> 3 4 5 <e> Pred: <s> four hundred ninety nine <e> Target: <s> three hundred forty five <e> \n",
      "Input: <s> 4 4 5 <e> Pred: <s> four hundred twenty nine <e> Target: <s> four hundred forty five <e> \n",
      "Input: <s> 7 4 9 <e> Pred: <s> seven hundred twenty nine <e> Target: <s> seven hundred forty nine <e> \n",
      "Input: <s> 9 7 8 <e> Pred: <s> four hundred twenty nine <e> Target: <s> nine hundred seventy eight <e> \n",
      "Input: <s> 4 1 3 <e> Pred: <s> seven hundred four <e> <pad> Target: <s> four hundred thirteen <e> <pad> \n",
      "Input: <s> 8 8 8 <e> Pred: <s> nine hundred ninety nine <e> Target: <s> eight hundred eighty eight <e> \n",
      "Input: <s> 5 2 3 <e> Pred: <s> four hundred twenty nine <e> Target: <s> five hundred twenty three <e> \n",
      "Input: <s> 5 6 4 <e> Pred: <s> nine hundred ninety nine <e> Target: <s> five hundred sixty four <e> \n",
      "Input: <s> 6 1 <e> <pad> Pred: <s> eighty one <e> <pad> <pad> Target: <s> sixty one <e> <pad> <pad> \n",
      "Input: <s> 3 1 7 <e> Pred: <s> nine hundred eight <e> <pad> Target: <s> three hundred seventeen <e> <pad> \n",
      "Input: <s> 4 4 2 <e> Pred: <s> four hundred ninety seven <e> Target: <s> four hundred forty two <e> \n",
      "Input: <s> 7 3 6 <e> Pred: <s> seven hundred twenty six <e> Target: <s> seven hundred thirty six <e> \n",
      "Input: <s> 2 9 8 <e> Pred: <s> nine hundred ninety nine <e> Target: <s> two hundred ninety eight <e> \n",
      "Input: <s> 1 1 4 <e> Pred: <s> one hundred sixteen <e> <pad> Target: <s> one hundred fourteen <e> <pad> \n",
      "Input: <s> 2 8 3 <e> Pred: <s> nine hundred twenty nine <e> Target: <s> two hundred eighty three <e> \n",
      "Input: <s> 6 2 6 <e> Pred: <s> nine hundred ninety six <e> Target: <s> six hundred twenty six <e> \n",
      "Input: <s> 6 4 6 <e> Pred: <s> nine hundred ninety six <e> Target: <s> six hundred forty six <e> \n",
      "Input: <s> 2 7 6 <e> Pred: <s> seven hundred twenty six <e> Target: <s> two hundred seventy six <e> \n",
      "Input: <s> 8 5 3 <e> Pred: <s> nine hundred ninety nine <e> Target: <s> eight hundred fifty three <e> \n",
      "Input: <s> 7 0 0 <e> Pred: <s> two hundred <e> <pad> <pad> Target: <s> seven hundred <e> <pad> <pad> \n",
      "Input: <s> 4 0 4 <e> Pred: <s> seven hundred four <e> <pad> Target: <s> four hundred four <e> <pad> \n",
      "Input: <s> 8 0 1 <e> Pred: <s> nine hundred one <e> <pad> Target: <s> eight hundred one <e> <pad> \n",
      "Input: <s> 2 6 <e> <pad> Pred: <s> thirty three <e> <pad> <pad> Target: <s> twenty six <e> <pad> <pad> \n",
      "Input: <s> 2 0 1 <e> Pred: <s> nine hundred one <e> <pad> Target: <s> two hundred one <e> <pad> \n",
      "Input: <s> 1 3 3 <e> Pred: <s> one hundred twenty four <e> Target: <s> one hundred thirty three <e> \n",
      "Input: <s> 8 4 6 <e> Pred: <s> nine hundred ninety two <e> Target: <s> eight hundred forty six <e> \n",
      "Input: <s> 2 6 6 <e> Pred: <s> nine hundred ninety six <e> Target: <s> two hundred sixty six <e> \n",
      "Input: <s> 3 5 8 <e> Pred: <s> nine hundred ninety nine <e> Target: <s> three hundred fifty eight <e> \n",
      "Input: <s> 9 2 6 <e> Pred: <s> nine hundred ninety two <e> Target: <s> nine hundred twenty six <e> \n",
      "Input: <s> 4 4 <e> <pad> Pred: <s> fifty three <e> <pad> <pad> Target: <s> forty four <e> <pad> <pad> \n",
      "Input: <s> 4 2 7 <e> Pred: <s> nine hundred fifty seven <e> Target: <s> four hundred twenty seven <e> \n",
      "Input: <s> 7 4 <e> <pad> Pred: <s> fifty seven <e> <pad> <pad> Target: <s> seventy four <e> <pad> <pad> \n",
      "Input: <s> 9 4 <e> <pad> Pred: <s> thirty three <e> <pad> <pad> Target: <s> ninety four <e> <pad> <pad> \n",
      "Input: <s> 9 3 5 <e> Pred: <s> nine hundred ninety nine <e> Target: <s> nine hundred thirty five <e> \n",
      "Input: <s> 1 9 1 <e> Pred: <s> one hundred twenty one <e> Target: <s> one hundred ninety one <e> \n",
      "Input: <s> 8 8 4 <e> Pred: <s> nine hundred ninety nine <e> Target: <s> eight hundred eighty four <e> \n",
      "Input: <s> 8 4 5 <e> Pred: <s> four hundred ninety nine <e> Target: <s> eight hundred forty five <e> \n",
      "Input: <s> 7 8 2 <e> Pred: <s> seven hundred twenty two <e> Target: <s> seven hundred eighty two <e> \n",
      "Input: <s> 7 7 8 <e> Pred: <s> one hundred twenty four <e> Target: <s> seven hundred seventy eight <e> \n",
      "Input: <s> 3 6 0 <e> Pred: <s> nine hundred fifty  <e> Target: <s> three hundred sixty  <e> \n",
      "Input: <s> 1 3 8 <e> Pred: <s> one hundred twenty four <e> Target: <s> one hundred thirty eight <e> \n",
      "Input: <s> 7 6 8 <e> Pred: <s> seven hundred twenty nine <e> Target: <s> seven hundred sixty eight <e> \n",
      "Input: <s> 2 2 7 <e> Pred: <s> seven hundred twenty seven <e> Target: <s> two hundred twenty seven <e> \n",
      "Input: <s> 2 8 4 <e> Pred: <s> four hundred sixty nine <e> Target: <s> two hundred eighty four <e> \n",
      "Input: <s> 3 1 8 <e> Pred: <s> nine hundred four <e> <pad> Target: <s> three hundred eighteen <e> <pad> \n",
      "Input: <s> 2 6 1 <e> Pred: <s> nine hundred twenty one <e> Target: <s> two hundred sixty one <e> \n",
      "Input: <s> 9 4 7 <e> Pred: <s> nine hundred ninety seven <e> Target: <s> nine hundred forty seven <e> \n",
      "Input: <s> 6 9 4 <e> Pred: <s> nine hundred ninety nine <e> Target: <s> six hundred ninety four <e> \n",
      "Input: <s> 7 0 1 <e> Pred: <s> seven hundred one <e> <pad> Target: <s> seven hundred one <e> <pad> \n",
      "Input: <s> 4 1 8 <e> Pred: <s> seven hundred four <e> <pad> Target: <s> four hundred eighteen <e> <pad> \n",
      "Input: <s> 4 2 2 <e> Pred: <s> seven hundred twenty two <e> Target: <s> four hundred twenty two <e> \n",
      "Input: <s> 2 1 7 <e> Pred: <s> nine hundred four <e> <pad> Target: <s> two hundred seventeen <e> <pad> \n",
      "Input: <s> 6 7 7 <e> Pred: <s> nine hundred fifty seven <e> Target: <s> six hundred seventy seven <e> \n",
      "Input: <s> 3 7 8 <e> Pred: <s> seven hundred twenty nine <e> Target: <s> three hundred seventy eight <e> \n",
      "Input: <s> 1 6 5 <e> Pred: <s> one hundred twenty four <e> Target: <s> one hundred sixty five <e> \n",
      "Input: <s> 1 3 1 <e> Pred: <s> one hundred twenty one <e> Target: <s> one hundred thirty one <e> \n",
      "Input: <s> 6 7 0 <e> Pred: <s> nine hundred fifty  <e> Target: <s> six hundred seventy  <e> \n",
      "Input: <s> 2 1 3 <e> Pred: <s> seven hundred four <e> <pad> Target: <s> two hundred thirteen <e> <pad> \n",
      "Input: <s> 2 4 4 <e> Pred: <s> seven hundred twenty nine <e> Target: <s> two hundred forty four <e> \n",
      "Input: <s> 2 0 <e> <pad> Pred: <s> <e>  <e> <pad> <pad> Target: <s> twenty  <e> <pad> <pad> \n",
      "Input: <s> 1 1 <e> <pad> Pred: <s> <e> <e> <pad> <pad> <pad> Target: <s> eleven <e> <pad> <pad> <pad> \n",
      "Input: <s> 5 7 6 <e> Pred: <s> seven hundred fifty six <e> Target: <s> five hundred seventy six <e> \n",
      "Input: <s> 9 2 3 <e> Pred: <s> nine hundred twenty nine <e> Target: <s> nine hundred twenty three <e> \n",
      "Input: <s> 2 1 2 <e> Pred: <s> nine hundred four <e> <pad> Target: <s> two hundred twelve <e> <pad> \n",
      "Input: <s> 5 1 2 <e> Pred: <s> nine hundred four <e> <pad> Target: <s> five hundred twelve <e> <pad> \n",
      "Input: <s> 1 0 7 <e> Pred: <s> one hundred one <e> <pad> Target: <s> one hundred seven <e> <pad> \n",
      "Input: <s> 6 3 0 <e> Pred: <s> nine hundred fifty  <e> Target: <s> six hundred thirty  <e> \n",
      "Input: <s> 3 7 7 <e> Pred: <s> seven hundred fifty seven <e> Target: <s> three hundred seventy seven <e> \n",
      "Input: <s> 5 8 6 <e> Pred: <s> nine hundred ninety six <e> Target: <s> five hundred eighty six <e> \n",
      "Input: <s> 1 8 6 <e> Pred: <s> one hundred twenty six <e> Target: <s> one hundred eighty six <e> \n",
      "Input: <s> 7 0 3 <e> Pred: <s> one hundred four <e> <pad> Target: <s> seven hundred three <e> <pad> \n",
      "Input: <s> 6 1 8 <e> Pred: <s> nine hundred four <e> <pad> Target: <s> six hundred eighteen <e> <pad> \n",
      "Input: <s> 4 8 6 <e> Pred: <s> nine hundred ninety six <e> Target: <s> four hundred eighty six <e> \n",
      "Input: <s> 8 3 6 <e> Pred: <s> nine hundred ninety two <e> Target: <s> eight hundred thirty six <e> \n",
      "Input: <s> 5 8 2 <e> Pred: <s> nine hundred ninety two <e> Target: <s> five hundred eighty two <e> \n",
      "Input: <s> 4 4 8 <e> Pred: <s> four hundred twenty nine <e> Target: <s> four hundred forty eight <e> \n",
      "Input: <s> 6 7 5 <e> Pred: <s> seven hundred twenty nine <e> Target: <s> six hundred seventy five <e> \n",
      "Input: <s> 1 8 8 <e> Pred: <s> one hundred twenty four <e> Target: <s> one hundred eighty eight <e> \n",
      "Input: <s> 8 4 8 <e> Pred: <s> nine hundred ninety nine <e> Target: <s> eight hundred forty eight <e> \n",
      "Input: <s> 3 1 3 <e> Pred: <s> nine hundred sixteen <e> <pad> Target: <s> three hundred thirteen <e> <pad> \n",
      "Input: <s> 8 3 3 <e> Pred: <s> nine hundred ninety nine <e> Target: <s> eight hundred thirty three <e> \n"
     ]
    }
   ],
   "source": [
    "# Evaluation phase\n",
    "count = 0\n",
    "\n",
    "for index, (inp, targ) in enumerate(evalloader):\n",
    "\n",
    "    bsize = inp.shape[0]\n",
    "    output = model(inp.float(), targ.float(), bsize, 0)\n",
    "\n",
    "    #print(\"model output:\", output.shape)\n",
    "    output = output.squeeze(1)\n",
    "    targ = torch.argmax(targ, dim=2).squeeze(0)\n",
    "\n",
    "    outr = output.view(-1,TAR_VOCAB_SIZE)\n",
    "    tarr = targ.view(-1)\n",
    "    loss += criterion(outr,tarr)\n",
    "\n",
    "    omax = torch.argmax(output, dim=2)\n",
    "    imax = torch.argmax(inp, dim=2)\n",
    "\n",
    "    outr = output.reshape(-1,TAR_VOCAB_SIZE)\n",
    "    tarr = targ.view(-1)\n",
    "\n",
    "    for b in range(bsize):\n",
    "\n",
    "        print(\"Input: \", end=\"\")\n",
    "        for i in range(INP_SEQ_LEN):\n",
    "            #print(f\"b:{b} s:{s}\")\n",
    "            print(inp_lang.idx2word[imax[b][i].item()] + \" \", end='')\n",
    "\n",
    "        print(\"Pred: \", end=\"\")\n",
    "        for t in range(TAR_SEQ_LEN):\n",
    "            #print(f\"b:{b} t:{t}\")\n",
    "            #print(\"omax entry:\", omax[b][t].item())\n",
    "            print(targ_lang.idx2word[omax[b][t].item()] + \" \", end='')\n",
    "\n",
    "        print(\"Target: \", end=\"\")\n",
    "        for s in range(TAR_SEQ_LEN):\n",
    "            #print(f\"b:{b} s:{s}\")\n",
    "            print(targ_lang.idx2word[targ[b][s].item()] + \" \", end='')\n",
    "\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaa2ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
