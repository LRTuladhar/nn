{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a6ec68b",
   "metadata": {},
   "source": [
    "Basic simple encoder decoder net.\n",
    "Could be expanded to become a fully fledged translator.\n",
    "\n",
    "This should be able to translate any set of token pairs. The first column should contain the input and the second column the target.\n",
    "The input file has a sequence of digits mapped to the number spelled out in words. \n",
    "Improvement ideas:\n",
    "* Use embeddings instead of one hot vector representation\n",
    "* Do not pass the target to the seq2seq during evaluation\n",
    "* Try using teacher forcing some % of the time\n",
    "* Generates the max length sequence - maybe stop when hit <end>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "04c4bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "c962afba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "filename = 'nums.txt'  # Replace with your file name\n",
    "\n",
    "nums =[]\n",
    "words=[]\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        nums.append(row[0])\n",
    "        words.append(row[1])\n",
    "\n",
    "zipped = zip(nums, words)\n",
    "\n",
    "pairs=[]\n",
    "for item in zipped:\n",
    "    n,w = item    \n",
    "    pairs.append([\"<s> \"+n+\" <e>\",\"<s> \"+w+\" <e>\"])\n",
    "    \n",
    "print(\"Num examples:\", len(pairs))\n",
    "\n",
    "\n",
    "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
    "# (e.g., 5 -> \"dad\") for each language,\n",
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        self.create_index()\n",
    "\n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "\n",
    "        self.vocab = sorted(self.vocab)\n",
    "\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        \n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word\n",
    "            \n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def load_dataset(pairs, num_examples):\n",
    "    # pairs => already created cleaned input, output pairs\n",
    "\n",
    "    # index language using the class defined above\n",
    "    inp_lang = LanguageIndex(n for n, w in pairs)\n",
    "    targ_lang = LanguageIndex(w for n, w in pairs)\n",
    "\n",
    "    # Vectorize the input and target languages\n",
    "\n",
    "    # Input sentences\n",
    "    input_tensor = [torch.tensor([inp_lang.word2idx[s] for s in en.split(' ')], dtype=torch.long) for en, ma in pairs]\n",
    "\n",
    "    # Target sentences\n",
    "    target_tensor = [torch.tensor([targ_lang.word2idx[s] for s in ma.split(' ')], dtype=torch.long) for en, ma in pairs]\n",
    "\n",
    "    # Calculate max_length of input and output tensor\n",
    "    # Here, we'll set those to the longest sentence in the dataset\n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "\n",
    "    # Padding the input and output tensor to the maximum length\n",
    "    input_tensor = pad_sequence(input_tensor, batch_first=True, padding_value=0)\n",
    "    input_tensor = input_tensor[:, :max_length_inp]\n",
    "\n",
    "    target_tensor = pad_sequence(target_tensor, batch_first=True, padding_value=0)\n",
    "    target_tensor = target_tensor[:, :max_length_tar]\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar\n",
    "\n",
    "\n",
    "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(pairs, len(pairs))\n",
    "\n",
    "# One hot tensors -> could move to an embedding layer\n",
    "oh_input = F.one_hot(input_tensor)\n",
    "oh_target = F.one_hot(target_tensor)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "cbfc46cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT VOCAB SIZE:14, TARGET VOCAB SIZE: 33, UNITS: 128,        INPUT SEQLEN:5, TARGET SEQ LENGTH: 6\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS AND HYPERPARAMETERS \n",
    "INP_VOCAB_SIZE = len(inp_lang.word2idx)\n",
    "TAR_VOCAB_SIZE = len(targ_lang.word2idx)\n",
    "UNITS = 128 # #of units in the GRU - both encoder and decoder use this setting\n",
    "INP_SEQ_LEN = max_length_inp\n",
    "TAR_SEQ_LEN = max_length_targ\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "print(f\"INPUT VOCAB SIZE:{INP_VOCAB_SIZE}, TARGET VOCAB SIZE: {TAR_VOCAB_SIZE}, UNITS: {UNITS},\\\n",
    "        INPUT SEQLEN:{INP_SEQ_LEN}, TARGET SEQ LENGTH: {TAR_SEQ_LEN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "28ab30b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.055296 M parameters\n",
      "0.066849 M parameters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Wrap the tensors in a TensorDataset and create the loader\n",
    "dataset = TensorDataset(oh_input, oh_target)\n",
    "\n",
    "# Split the dataset into train and eval subsets\n",
    "train_size = int(0.9 * len(dataset))  # 90% for training\n",
    "eval_size = len(dataset) - train_size  # 10% for evaluation\n",
    "train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "# Create DataLoaders for training and evaluation\n",
    "dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "evalloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "# Encoder class \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Encoder GRU\n",
    "        self.encoder_gru = nn.GRU(input_size, hidden_size, batch_first=True)    \n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # input_seq: [batch, seq, input_vocab]\n",
    "        # outout : [batch, seq, units]\n",
    "        # hidden: [1, seq, units] or [numlayers, seq, units]\n",
    "        encoder_hidden = self.initialize_hidden(input_seq.shape[0])\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_output, encoder_hidden = self.encoder_gru(input_seq, encoder_hidden)\n",
    "        return encoder_output, encoder_hidden\n",
    "    \n",
    "    def initialize_hidden(self, batchlen):\n",
    "        return torch.zeros(1, batchlen, self.hidden_size)\n",
    "\n",
    "# Decoder class\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Decoder GRU\n",
    "        self.decoder_gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        # Linear layer to map hidden state to output\n",
    "        self.fc = nn.Linear(hidden_size, output_size) \n",
    "\n",
    "    def forward(self, input_seq, hidden):\n",
    "        # input_seq: [batch, seq (1), input_vocab]\n",
    "        # outout : [batch, seq, units]\n",
    "        # hidden: [1, seq, units] or [numlayers, seq, units]\n",
    "\n",
    "        dec_output, dec_hidden = self.decoder_gru(input_seq, hidden)\n",
    "        # dec_output shape: 1, seq_len, hidden_size \n",
    "        \n",
    "        dec_output = self.fc(dec_output)\n",
    "        # output shape: 1, seq_len, output_size\n",
    "        return dec_output, dec_hidden\n",
    "\n",
    "    \n",
    "# The Seq2Seq model to connect the encoder and decoder\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    \n",
    "    def forward(self, source, target, batchsize, training):\n",
    "        # The sequence we produce will be stored in this tensor\n",
    "        outputs = torch.zeros(batchsize, TAR_SEQ_LEN , TAR_VOCAB_SIZE)\n",
    "        #print(\"outputs storage shape:\", outputs.shape)\n",
    "        \n",
    "        # Call the encoder\n",
    "        enc_output, enc_hidden = self.encoder(source)\n",
    "        # Grab the last hidden layer for the decoder\n",
    "        # enc_output shape: [batch, seq, units]\n",
    "        # hidden shape: [1, seq, units] or [numlayers, seq, units] \n",
    "        \n",
    "        # Grab the last time slice of the hidden units\n",
    "        hidden = enc_hidden\n",
    "        # hidden shape: [1, seq, units]\n",
    "        \n",
    "        # Grab the first input to the Decoder which will be the <start> token\n",
    "        output = seed_oh = F.one_hot(torch.tensor(targ_lang.word2idx['<s>']), TAR_VOCAB_SIZE).unsqueeze(0).float()\n",
    "        output = output.unsqueeze(0)\n",
    "        ##print(\"output shape A:\", output.shape)\n",
    "        output = output.repeat(batchsize,1,1)\n",
    "        ##print(\"output shape B:\", output.shape)\n",
    "        ##print(\"target shape:\", target.shape)\n",
    "        \n",
    "        # Generate the full sequence -> loop could be improved to a while loop\n",
    "        for t in range(TAR_SEQ_LEN):\n",
    "            #if(training):\n",
    "            #    #Teacher forcing only if during training\n",
    "            #    x = target[:,t,:].unsqueeze(1)\n",
    "            #    #print(\"Train x shape:\", x.shape)\n",
    "            #else:\n",
    "            x = output\n",
    "            ##print(\"x shape in loop:\", x.shape)\n",
    "            ##print(\"hidden shape in loop:\", hidden.shape)\n",
    "\n",
    "            # Use previous hidden, cell as context from encoder at start\n",
    "            output, hidden = self.decoder(x, hidden)\n",
    "            ##print(\"dec output shape:\", output.shape)\n",
    "            ##print(\"dec hidden shape:\", hidden.shape)\n",
    "            # output shape: 1, seq_len, output_size\n",
    "            \n",
    "            # Store next output prediction\n",
    "            outputs[:,t,:] = output.squeeze()           \n",
    "            # outputs shape: seq_len, output_size\n",
    "            ##print(\"output[] shape:\", outputs.shape)\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "    \n",
    "# Build the model and print out some stats    \n",
    "encoder = Encoder(input_size=INP_VOCAB_SIZE, hidden_size=UNITS)\n",
    "decoder = Decoder(input_size=TAR_VOCAB_SIZE, hidden_size=UNITS, output_size=TAR_VOCAB_SIZE)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "\n",
    "print(sum(p.numel() for p in encoder.parameters())/1e6, 'M parameters')\n",
    "print(sum(p.numel() for p in decoder.parameters())/1e6, 'M parameters')\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "37435da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 52.58573532104492\n",
      "Loss: 27.98406982421875\n",
      "Loss: 22.918245315551758\n",
      "Loss: 20.16581916809082\n",
      "Loss: 18.225666046142578\n",
      "Loss: 16.084938049316406\n",
      "Loss: 15.08467960357666\n",
      "Loss: 14.502851486206055\n",
      "Loss: 13.335037231445312\n",
      "Loss: 12.06436824798584\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Training - clean\n",
    "\n",
    "# for param_group in optimizer.param_groups:\n",
    "#     param_group['lr'] = 0.01\n",
    "\n",
    "bsize = 1\n",
    "\n",
    "EPOCHS = 500\n",
    "model.train()\n",
    "\n",
    "for e in range(EPOCHS):    \n",
    "    loss =0 \n",
    "    for index, (inp, targ) in enumerate(dataloader):\n",
    "\n",
    "        bsize = inp.shape[0]\n",
    "        \n",
    "        output = model(inp.float(), targ.float(), bsize, 1)\n",
    "        \n",
    "        #print(\"model output:\", output.shape)\n",
    "        output = output.squeeze(1)\n",
    "        targ = torch.argmax(targ, dim=2).squeeze(0)\n",
    "        \n",
    "        outr = output.view(-1,TAR_VOCAB_SIZE)\n",
    "        tarr = targ.view(-1)\n",
    "        loss += criterion(outr,tarr)\n",
    "\n",
    "    if(e%(EPOCHS/10)==0):\n",
    "        print(\"Loss:\", loss.item())\n",
    "\n",
    "    # Training time ...\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "64152c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <s> 9 6 4 <end> Pred: <start> nine hundred twenty seven <e> Target: <start> nine hundred sixty four <e> \n",
      "Input: <s> 2 1 7 <end> Pred: <start> nine hundred four <e> <pad> Target: <start> two hundred seventeen <e> <pad> \n",
      "Input: <s> 5 4 2 <end> Pred: <start> four hundred forty two <e> Target: <start> five hundred forty two <e> \n",
      "Input: <s> 9 1 9 <end> Pred: <start> nine hundred eleven <e> <pad> Target: <start> nine hundred nineteen <e> <pad> \n",
      "Input: <s> 4 5 4 <end> Pred: <start> four hundred forty six <e> Target: <start> four hundred fifty four <e> \n",
      "Input: <s> 5 3 <end> <pad> Pred: <start> eighty seven <e> <pad> <pad> Target: <start> fifty three <e> <pad> <pad> \n",
      "Input: <s> 7 3 4 <end> Pred: <start> six hundred twenty seven <e> Target: <start> seven hundred thirty four <e> \n",
      "Input: <s> 4 1 2 <end> Pred: <start> eight hundred six <e> <pad> Target: <start> four hundred twelve <e> <pad> \n",
      "Input: <s> 6 3 4 <end> Pred: <start> six hundred twenty seven <e> Target: <start> six hundred thirty four <e> \n",
      "Input: <s> 3 8 3 <end> Pred: <start> six hundred twenty three <e> Target: <start> three hundred eighty three <e> \n",
      "Input: <s> 2 2 2 <end> Pred: <start> two hundred twenty two <e> Target: <start> two hundred twenty two <e> \n",
      "Input: <s> 7 7 9 <end> Pred: <start> eight hundred seventy nine <e> Target: <start> seven hundred seventy nine <e> \n",
      "Input: <s> 4 3 <end> <pad> Pred: <start> eighty seven <e> <pad> <pad> Target: <start> forty three <e> <pad> <pad> \n",
      "Input: <s> 4 7 4 <end> Pred: <start> four hundred fifty six <e> Target: <start> four hundred seventy four <e> \n",
      "Input: <s> 7 1 5 <end> Pred: <start> six hundred ninety <e> <pad> Target: <start> seven hundred fifteen <e> <pad> \n",
      "Input: <s> 8 5 1 <end> Pred: <start> four hundred seventy one <e> Target: <start> eight hundred fifty one <e> \n",
      "Input: <s> 2 8 7 <end> Pred: <start> six hundred twenty six <e> Target: <start> two hundred eighty seven <e> \n",
      "Input: <s> 6 0 6 <end> Pred: <start> six hundred six <e> <pad> Target: <start> six hundred six <e> <pad> \n",
      "Input: <s> 7 1 6 <end> Pred: <start> six hundred four <e> <pad> Target: <start> seven hundred sixteen <e> <pad> \n",
      "Input: <s> 9 8 9 <end> Pred: <start> nine hundred twenty nine <e> Target: <start> nine hundred eighty nine <e> \n",
      "Input: <s> 4 4 7 <end> Pred: <start> four hundred forty six <e> Target: <start> four hundred forty seven <e> \n",
      "Input: <s> 9 0 7 <end> Pred: <start> nine hundred eleven <e> <pad> Target: <start> nine hundred seven <e> <pad> \n",
      "Input: <s> 2 3 2 <end> Pred: <start> two hundred twenty two <e> Target: <start> two hundred thirty two <e> \n",
      "Input: <s> 9 9 1 <end> Pred: <start> one hundred twenty nine <e> Target: <start> nine hundred ninety one <e> \n",
      "Input: <s> 5 8 0 <end> Pred: <start> four hundred forty  <e> Target: <start> five hundred eighty  <e> \n",
      "Input: <s> 4 1 0 <end> Pred: <start> eight hundred one <e> <pad> Target: <start> four hundred ten <e> <pad> \n",
      "Input: <s> 1 4 1 <end> Pred: <start> one hundred ninety nine <e> Target: <start> one hundred forty one <e> \n",
      "Input: <s> 5 7 0 <end> Pred: <start> four hundred forty  <e> Target: <start> five hundred seventy  <e> \n",
      "Input: <s> 9 3 4 <end> Pred: <start> nine hundred twenty seven <e> Target: <start> nine hundred thirty four <e> \n",
      "Input: <s> 1 0 9 <end> Pred: <start> one hundred <e> <e> <pad> Target: <start> one hundred nine <e> <pad> \n",
      "Input: <s> 7 3 1 <end> Pred: <start> six hundred seventy one <e> Target: <start> seven hundred thirty one <e> \n",
      "Input: <s> 4 0 5 <end> Pred: <start> four hundred four <e> <pad> Target: <start> four hundred five <e> <pad> \n",
      "Input: <s> 5 5 1 <end> Pred: <start> four hundred forty one <e> Target: <start> five hundred fifty one <e> \n",
      "Input: <s> 2 3 3 <end> Pred: <start> two hundred twenty three <e> Target: <start> two hundred thirty three <e> \n",
      "Input: <s> 8 8 5 <end> Pred: <start> five hundred forty six <e> Target: <start> eight hundred eighty five <e> \n",
      "Input: <s> 7 0 8 <end> Pred: <start> eight hundred four <e> <pad> Target: <start> seven hundred eight <e> <pad> \n",
      "Input: <s> 7 9 6 <end> Pred: <start> two hundred twenty six <e> Target: <start> seven hundred ninety six <e> \n",
      "Input: <s> 5 9 1 <end> Pred: <start> six hundred twenty one <e> Target: <start> five hundred ninety one <e> \n",
      "Input: <s> 8 9 8 <end> Pred: <start> six hundred twenty seven <e> Target: <start> eight hundred ninety eight <e> \n",
      "Input: <s> 3 3 <end> <pad> Pred: <start> sixty two <e> <pad> <pad> Target: <start> thirty three <e> <pad> <pad> \n",
      "Input: <s> 8 4 4 <end> Pred: <start> four hundred forty six <e> Target: <start> eight hundred forty four <e> \n",
      "Input: <s> 8 6 3 <end> Pred: <start> eight hundred seventy three <e> Target: <start> eight hundred sixty three <e> \n",
      "Input: <s> 2 1 8 <end> Pred: <start> one hundred ninety <e> <pad> Target: <start> two hundred eighteen <e> <pad> \n",
      "Input: <s> 3 0 <end> <pad> Pred: <start> sixty one <e> <pad> <pad> Target: <start> thirty  <e> <pad> <pad> \n",
      "Input: <s> 1 5 7 <end> Pred: <start> one hundred twenty six <e> Target: <start> one hundred fifty seven <e> \n",
      "Input: <s> 7 1 2 <end> Pred: <start> six hundred six <e> <pad> Target: <start> seven hundred twelve <e> <pad> \n",
      "Input: <s> 8 4 5 <end> Pred: <start> five hundred forty six <e> Target: <start> eight hundred forty five <e> \n",
      "Input: <s> 8 6 4 <end> Pred: <start> eight hundred fifty seven <e> Target: <start> eight hundred sixty four <e> \n",
      "Input: <s> 7 2 9 <end> Pred: <start> six hundred twenty nine <e> Target: <start> seven hundred twenty nine <e> \n",
      "Input: <s> 1 3 6 <end> Pred: <start> one hundred twenty two <e> Target: <start> one hundred thirty six <e> \n",
      "Input: <s> 2 7 5 <end> Pred: <start> three hundred twenty seven <e> Target: <start> two hundred seventy five <e> \n",
      "Input: <s> 3 1 2 <end> Pred: <start> two hundred eleven <e> <pad> Target: <start> three hundred twelve <e> <pad> \n",
      "Input: <s> 1 5 4 <end> Pred: <start> one hundred twenty seven <e> Target: <start> one hundred fifty four <e> \n",
      "Input: <s> 5 2 1 <end> Pred: <start> eight hundred seventy one <e> Target: <start> five hundred twenty one <e> \n",
      "Input: <s> 2 5 8 <end> Pred: <start> six hundred twenty six <e> Target: <start> two hundred fifty eight <e> \n",
      "Input: <s> 3 7 2 <end> Pred: <start> three hundred twenty two <e> Target: <start> three hundred seventy two <e> \n",
      "Input: <s> 4 6 7 <end> Pred: <start> four hundred fifty six <e> Target: <start> four hundred sixty seven <e> \n",
      "Input: <s> 6 8 9 <end> Pred: <start> six hundred twenty nine <e> Target: <start> six hundred eighty nine <e> \n",
      "Input: <s> 8 2 7 <end> Pred: <start> eight hundred twenty six <e> Target: <start> eight hundred twenty seven <e> \n",
      "Input: <s> 8 2 4 <end> Pred: <start> eight hundred twenty seven <e> Target: <start> eight hundred twenty four <e> \n",
      "Input: <s> 3 4 3 <end> Pred: <start> seven hundred seventy three <e> Target: <start> three hundred forty three <e> \n",
      "Input: <s> 9 0 4 <end> Pred: <start> one hundred four <e> <pad> Target: <start> nine hundred four <e> <pad> \n",
      "Input: <s> 2 8 4 <end> Pred: <start> six hundred twenty seven <e> Target: <start> two hundred eighty four <e> \n",
      "Input: <s> 2 8 0 <end> Pred: <start> six hundred seventy one <e> Target: <start> two hundred eighty  <e> \n",
      "Input: <s> 3 1 6 <end> Pred: <start> nine hundred four <e> <pad> Target: <start> three hundred sixteen <e> <pad> \n",
      "Input: <s> 4 3 1 <end> Pred: <start> four hundred seventy one <e> Target: <start> four hundred thirty one <e> \n",
      "Input: <s> 8 3 0 <end> Pred: <start> eight hundred seventy  <e> Target: <start> eight hundred thirty  <e> \n",
      "Input: <s> 1 6 <end> <pad> Pred: <start> two <e> <pad> <pad> <pad> Target: <start> sixteen <e> <pad> <pad> <pad> \n",
      "Input: <s> 7 9 4 <end> Pred: <start> three hundred twenty seven <e> Target: <start> seven hundred ninety four <e> \n",
      "Input: <s> 1 2 5 <end> Pred: <start> one hundred twenty seven <e> Target: <start> one hundred twenty five <e> \n",
      "Input: <s> 7 5 9 <end> Pred: <start> four hundred seventy nine <e> Target: <start> seven hundred fifty nine <e> \n",
      "Input: <s> 5 2 4 <end> Pred: <start> four hundred fifty seven <e> Target: <start> five hundred twenty four <e> \n",
      "Input: <s> 9 7 3 <end> Pred: <start> nine hundred twenty three <e> Target: <start> nine hundred seventy three <e> \n",
      "Input: <s> 2 9 4 <end> Pred: <start> nine hundred twenty seven <e> Target: <start> two hundred ninety four <e> \n",
      "Input: <s> 2 9 6 <end> Pred: <start> nine hundred twenty six <e> Target: <start> two hundred ninety six <e> \n",
      "Input: <s> 7 2 7 <end> Pred: <start> six hundred twenty six <e> Target: <start> seven hundred twenty seven <e> \n",
      "Input: <s> 4 2 6 <end> Pred: <start> four hundred twenty two <e> Target: <start> four hundred twenty six <e> \n",
      "Input: <s> 6 4 0 <end> Pred: <start> four hundred seventy one <e> Target: <start> six hundred forty  <e> \n",
      "Input: <s> 3 8 1 <end> Pred: <start> six hundred seventy one <e> Target: <start> three hundred eighty one <e> \n",
      "Input: <s> 6 7 0 <end> Pred: <start> eight hundred seventy one <e> Target: <start> six hundred seventy  <e> \n",
      "Input: <s> 7 2 6 <end> Pred: <start> six hundred twenty six <e> Target: <start> seven hundred twenty six <e> \n",
      "Input: <s> 3 2 1 <end> Pred: <start> two hundred twenty one <e> Target: <start> three hundred twenty one <e> \n",
      "Input: <s> 7 1 7 <end> Pred: <start> six hundred four <e> <pad> Target: <start> seven hundred seventeen <e> <pad> \n",
      "Input: <s> 8 6 1 <end> Pred: <start> eight hundred seventy one <e> Target: <start> eight hundred sixty one <e> \n",
      "Input: <s> 4 5 9 <end> Pred: <start> four hundred forty nine <e> Target: <start> four hundred fifty nine <e> \n",
      "Input: <s> 5 5 9 <end> Pred: <start> four hundred forty nine <e> Target: <start> five hundred fifty nine <e> \n",
      "Input: <s> 8 7 6 <end> Pred: <start> eight hundred forty two <e> Target: <start> eight hundred seventy six <e> \n",
      "Input: <s> 7 1 4 <end> Pred: <start> six hundred ninety <e> <pad> Target: <start> seven hundred fourteen <e> <pad> \n",
      "Input: <s> 7 4 1 <end> Pred: <start> four hundred seventy one <e> Target: <start> seven hundred forty one <e> \n",
      "Input: <s> 5 8 <end> <pad> Pred: <start> hundred seven <e> <pad> <pad> Target: <start> fifty eight <e> <pad> <pad> \n",
      "Input: <s> 9 0 8 <end> Pred: <start> one hundred four <e> <pad> Target: <start> nine hundred eight <e> <pad> \n",
      "Input: <s> 7 8 3 <end> Pred: <start> eight hundred seventy two <e> Target: <start> seven hundred eighty three <e> \n",
      "Input: <s> 4 1 4 <end> Pred: <start> eight hundred fifty <e> <pad> Target: <start> four hundred fourteen <e> <pad> \n",
      "Input: <s> 9 9 8 <end> Pred: <start> one hundred twenty six <e> Target: <start> nine hundred ninety eight <e> \n",
      "Input: <s> 2 8 8 <end> Pred: <start> six hundred twenty seven <e> Target: <start> two hundred eighty eight <e> \n",
      "Input: <s> 3 3 2 <end> Pred: <start> three hundred twenty three <e> Target: <start> three hundred thirty two <e> \n",
      "Input: <s> 2 6 0 <end> Pred: <start> two hundred twenty one <e> Target: <start> two hundred sixty  <e> \n",
      "Input: <s> 4 7 7 <end> Pred: <start> five hundred forty six <e> Target: <start> four hundred seventy seven <e> \n",
      "Input: <s> 9 6 9 <end> Pred: <start> nine hundred twenty nine <e> Target: <start> nine hundred sixty nine <e> \n",
      "Input: <s> 2 1 9 <end> Pred: <start> two hundred eleven <e> <pad> Target: <start> two hundred nineteen <e> <pad> \n"
     ]
    }
   ],
   "source": [
    "\n",
    "count = 0\n",
    "\n",
    "for index, (inp, targ) in enumerate(evalloader):\n",
    "\n",
    "    bsize = inp.shape[0]\n",
    "    output = model(inp.float(), targ.float(), bsize, 0)\n",
    "\n",
    "    #print(\"model output:\", output.shape)\n",
    "    output = output.squeeze(1)\n",
    "    targ = torch.argmax(targ, dim=2).squeeze(0)\n",
    "\n",
    "    outr = output.view(-1,TAR_VOCAB_SIZE)\n",
    "    tarr = targ.view(-1)\n",
    "    loss += criterion(outr,tarr)\n",
    "\n",
    "    omax = torch.argmax(output, dim=2)\n",
    "    imax = torch.argmax(inp, dim=2)\n",
    "\n",
    "    outr = output.reshape(-1,TAR_VOCAB_SIZE)\n",
    "    tarr = targ.view(-1)\n",
    "\n",
    "    for b in range(bsize):\n",
    "\n",
    "        print(\"Input: \", end=\"\")\n",
    "        for i in range(INP_SEQ_LEN):\n",
    "            #print(f\"b:{b} s:{s}\")\n",
    "            print(inp_lang.idx2word[imax[b][i].item()] + \" \", end='')\n",
    "\n",
    "        print(\"Pred: \", end=\"\")\n",
    "        for t in range(TAR_SEQ_LEN):\n",
    "            #print(f\"b:{b} t:{t}\")\n",
    "            #print(\"omax entry:\", omax[b][t].item())\n",
    "            print(targ_lang.idx2word[omax[b][t].item()] + \" \", end='')\n",
    "\n",
    "        print(\"Target: \", end=\"\")\n",
    "        for s in range(TAR_SEQ_LEN):\n",
    "            #print(f\"b:{b} s:{s}\")\n",
    "            print(targ_lang.idx2word[targ[b][s].item()] + \" \", end='')\n",
    "\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d861a220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
